{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"credit: https://github.com/airsplay/py-bottom-up-attention","metadata":{}},{"cell_type":"code","source":"%%capture\n!git clone https://github.com/airsplay/py-bottom-up-attention.git\n%cd py-bottom-up-attention\n\n# Install python libraries\n!pip install -r requirements.txt\n!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n\n# Install detectron2\n!python setup.py build develop\n\n# or if you are on macOS\n# MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build develop\n\n# or, as an alternative to `setup.py`, do\n# pip install [--editable] .\n!pip install --upgrade --force-reinstall  imagesize","metadata":{"execution":{"iopub.status.busy":"2021-10-07T09:13:27.145546Z","iopub.execute_input":"2021-10-07T09:13:27.145921Z","iopub.status.idle":"2021-10-07T09:19:14.747063Z","shell.execute_reply.started":"2021-10-07T09:13:27.145823Z","shell.execute_reply":"2021-10-07T09:19:14.745846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nimport os\nimport io\nimport json\nimport detectron2\nfrom tqdm.notebook import tqdm\n\n# import some common detectron2 utilities\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog\n\n# import some common libraries\nimport numpy as np\nimport cv2\nimport torch\n\n# Show the image in ipynb\nfrom IPython.display import clear_output, Image, display\nimport PIL.Image\ndef showarray(a, fmt='jpeg'):\n    a = np.uint8(np.clip(a, 0, 255))\n    f = io.BytesIO()\n    PIL.Image.fromarray(a).save(f, fmt)\n    display(Image(data=f.getvalue()))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-07T09:54:27.077286Z","iopub.execute_input":"2021-10-07T09:54:27.078123Z","iopub.status.idle":"2021-10-07T09:54:27.086461Z","shell.execute_reply.started":"2021-10-07T09:54:27.078081Z","shell.execute_reply":"2021-10-07T09:54:27.085469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd demo\n!ls","metadata":{"execution":{"iopub.status.busy":"2021-10-07T09:25:44.669578Z","iopub.execute_input":"2021-10-07T09:25:44.669847Z","iopub.status.idle":"2021-10-07T09:25:45.360035Z","shell.execute_reply.started":"2021-10-07T09:25:44.669818Z","shell.execute_reply":"2021-10-07T09:25:45.359159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load VG Classes\ndata_path = 'data/genome/1600-400-20'\n\nvg_classes = []\nwith open(os.path.join(data_path, 'objects_vocab.txt')) as f:\n    for object in f.readlines():\n        vg_classes.append(object.split(',')[0].lower().strip())\n\nMetadataCatalog.get(\"vg\").thing_classes = vg_classes","metadata":{"execution":{"iopub.status.busy":"2021-10-07T09:26:06.992031Z","iopub.execute_input":"2021-10-07T09:26:06.992803Z","iopub.status.idle":"2021-10-07T09:26:07.000663Z","shell.execute_reply.started":"2021-10-07T09:26:06.992735Z","shell.execute_reply":"2021-10-07T09:26:06.999886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = get_cfg()\ncfg.merge_from_file(\"../configs/VG-Detection/faster_rcnn_R_101_C4_caffe.yaml\")\ncfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300\ncfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n# VG Weight\ncfg.MODEL.WEIGHTS = \"http://nlp.cs.unc.edu/models/faster_rcnn_from_caffe.pkl\"\npredictor = DefaultPredictor(cfg)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T07:39:25.031036Z","iopub.execute_input":"2021-10-05T07:39:25.031343Z","iopub.status.idle":"2021-10-05T07:39:36.978644Z","shell.execute_reply.started":"2021-10-05T07:39:25.031306Z","shell.execute_reply":"2021-10-05T07:39:36.977877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = get_cfg()\ncfg.merge_from_file(\"../configs/VG-Detection/faster_rcnn_R_101_C4_caffe.yaml\")\ncfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300\ncfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n# VG Weight\ncfg.MODEL.WEIGHTS = \"http://nlp.cs.unc.edu/models/faster_rcnn_from_caffe.pkl\"\npredictor = DefaultPredictor(cfg)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T09:26:21.656344Z","iopub.execute_input":"2021-10-07T09:26:21.657101Z","iopub.status.idle":"2021-10-07T09:26:36.523876Z","shell.execute_reply.started":"2021-10-07T09:26:21.657063Z","shell.execute_reply":"2021-10-07T09:26:36.523079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im_paths = glob('/kaggle/input/simmc-img/data/all_images/*.png')","metadata":{"execution":{"iopub.status.busy":"2021-10-07T09:47:22.64718Z","iopub.execute_input":"2021-10-07T09:47:22.64745Z","iopub.status.idle":"2021-10-07T09:47:22.661315Z","shell.execute_reply.started":"2021-10-07T09:47:22.64742Z","shell.execute_reply":"2021-10-07T09:47:22.660441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_OBJECTS = 10000\n\nfrom torch import nn\n\nfrom detectron2.modeling.postprocessing import detector_postprocess\nfrom detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference_single_image\nfrom detectron2.structures.boxes import Boxes\nfrom detectron2.structures.instances import Instances\n\ndef doit(raw_image, raw_boxes):\n        # Process Boxes\n    raw_boxes = Boxes(torch.from_numpy(raw_boxes).cuda())\n    \n    with torch.no_grad():\n        raw_height, raw_width = raw_image.shape[:2]\n#         print(\"Original image size: \", (raw_height, raw_width))\n        \n        # Preprocessing\n        image = predictor.transform_gen.get_transform(raw_image).apply_image(raw_image)\n#         print(\"Transformed image size: \", image.shape[:2])\n        \n        # Scale the box\n        new_height, new_width = image.shape[:2]\n        scale_x = 1. * new_width / raw_width\n        scale_y = 1. * new_height / raw_height\n        #print(scale_x, scale_y)\n        boxes = raw_boxes.clone()\n        boxes.scale(scale_x=scale_x, scale_y=scale_y)\n        \n        # ----\n        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n        inputs = [{\"image\": image, \"height\": raw_height, \"width\": raw_width}]\n        images = predictor.model.preprocess_image(inputs)\n        \n        # Run Backbone Res1-Res4\n        features = predictor.model.backbone(images.tensor)\n        \n        # Run RoI head for each proposal (RoI Pooling + Res5)\n        proposal_boxes = [boxes]\n        features = [features[f] for f in predictor.model.roi_heads.in_features]\n        box_features = predictor.model.roi_heads._shared_roi_transform(\n            features, proposal_boxes\n        )\n        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n#         print('Pooled features size:', feature_pooled.shape)\n        \n        # Predict classes and boxes for each proposal.\n        pred_class_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(feature_pooled)\n#         print(pred_class_logits.shape)\n        pred_class_prob = nn.functional.softmax(pred_class_logits, -1)\n        pred_scores, pred_classes = pred_class_prob[..., :-1].max(-1)\n        \n        # Detectron2 Formatting (for visualization only)\n        roi_features = feature_pooled\n        instances = Instances(\n            image_size=(raw_height, raw_width),\n            pred_boxes=raw_boxes,\n            scores=pred_scores,\n            pred_classes=pred_classes\n        )\n        \n        return instances, roi_features\n\ndef read_data(im_path, m=False):\n    try:\n        im = cv2.imread(im_path)\n        im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    except:\n        return None\n    h, w = im.shape[0], im.shape[1]\n    name = im_path.split('/')[-1].split('.')[0]\n    given_boxes = []\n    indices = []\n    try:\n        if not m:\n            with open(f'/kaggle/input/simmc-img/data/simmc2_scene_jsons_dstc10_public/public/{name}_scene.json') as f:\n                data = json.load(f)\n        else:\n            with open(f'/kaggle/input/simmc-img/data/simmc2_scene_jsons_dstc10_public/public/m_{name}_scene.json') as f:\n                data = json.load(f)\n    except:\n        return None\n    for obj in data['scenes'][0]['objects']:\n        x0 = obj['bbox'][0]\n        y0 = obj['bbox'][1]\n        x1 = x0 + obj['bbox'][3]\n        y1 = y0 + obj['bbox'][2]\n        given_boxes.append([x0, y0, x1, y1])\n        indices.append(obj['index'])\n    given_boxes.append([0, 0, w, h])\n    indices.append('scene')\n    return im, np.array(given_boxes), indices, name\n\n# instances, features = doit(im, given_boxes)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:13:51.430924Z","iopub.execute_input":"2021-10-07T10:13:51.431371Z","iopub.status.idle":"2021-10-07T10:13:51.461731Z","shell.execute_reply.started":"2021-10-07T10:13:51.431332Z","shell.execute_reply":"2021-10-07T10:13:51.460653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"without_m = {}\nwith_m = {}\nfor im_path in tqdm(im_paths):\n    data = read_data(im_path)\n    if data is not None:\n        im, given_boxes, indices, name = data\n        d = {}\n        instances, features = doit(im, given_boxes)\n        features = features.cpu().tolist()\n        for i, idx in enumerate(indices):\n            d[idx] = features[i]\n        without_m[name] = d\nfor im_path in tqdm(im_paths):\n    data = read_data(im_path, m=True)\n    if data is not None:\n        im, given_boxes, indices, name = data\n        d = {}\n        instances, features = doit(im, given_boxes)\n        features = features.cpu().tolist()\n        for i, idx in enumerate(indices):\n            d[idx] = features[i]\n        with_m[name] = d","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:14:11.004588Z","iopub.execute_input":"2021-10-07T10:14:11.005226Z","iopub.status.idle":"2021-10-07T10:21:03.565176Z","shell.execute_reply.started":"2021-10-07T10:14:11.005186Z","shell.execute_reply":"2021-10-07T10:21:03.562857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(without_m), len(with_m)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:21:06.77404Z","iopub.execute_input":"2021-10-07T10:21:06.774292Z","iopub.status.idle":"2021-10-07T10:21:06.779972Z","shell.execute_reply.started":"2021-10-07T10:21:06.774264Z","shell.execute_reply":"2021-10-07T10:21:06.779228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n!rm -r *","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:26:17.305553Z","iopub.execute_input":"2021-10-07T10:26:17.306217Z","iopub.status.idle":"2021-10-07T10:26:18.309147Z","shell.execute_reply.started":"2021-10-07T10:26:17.306176Z","shell.execute_reply":"2021-10-07T10:26:18.308278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:26:37.168073Z","iopub.execute_input":"2021-10-07T10:26:37.168361Z","iopub.status.idle":"2021-10-07T10:26:37.987987Z","shell.execute_reply.started":"2021-10-07T10:26:37.168328Z","shell.execute_reply":"2021-10-07T10:26:37.987003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('without_m.json', 'w') as f:\n    json.dump(without_m, f)\nwith open('with_m.json', 'w') as f:\n    json.dump(with_m, f)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:26:42.633283Z","iopub.execute_input":"2021-10-07T10:26:42.633633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the boxes, labels, and features\npred = instances.to('cpu')\nv = Visualizer(im[:, :, :], MetadataCatalog.get(\"vg\"), scale=1.2)\nv = v.draw_instance_predictions(pred)\nshowarray(v.get_image()[:, :, ::-1])\nprint('instances:\\n', instances)\nprint()\nprint('boxes:\\n', instances.pred_boxes)\nprint()\n# print('Shape of features:\\n', features.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:12:37.998908Z","iopub.execute_input":"2021-10-07T10:12:37.999248Z","iopub.status.idle":"2021-10-07T10:12:38.501333Z","shell.execute_reply.started":"2021-10-07T10:12:37.999215Z","shell.execute_reply":"2021-10-07T10:12:38.500668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Verify the correspondence of RoI features\n# pred_class_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(features)\n# pred_class_probs = torch.nn.functional.softmax(pred_class_logits, -1)[:, :-1]\n# max_probs, max_classes = pred_class_probs.max(-1)\n# print(\"%d objects are different, it is because the classes-aware NMS process\" % (NUM_OBJECTS - torch.eq(instances.pred_classes, max_classes).sum().item()))\n# print(\"The total difference of score is %0.4f\" % (instances.scores - max_probs).abs().sum().item())","metadata":{"execution":{"iopub.status.busy":"2021-10-07T10:12:50.294244Z","iopub.execute_input":"2021-10-07T10:12:50.294506Z","iopub.status.idle":"2021-10-07T10:12:50.297934Z","shell.execute_reply.started":"2021-10-07T10:12:50.294473Z","shell.execute_reply":"2021-10-07T10:12:50.297252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}